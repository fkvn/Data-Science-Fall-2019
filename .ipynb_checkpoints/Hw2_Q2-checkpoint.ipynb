{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports all necessary library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import KNeighborsClassifier \"Class\"\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import numpy and pandas\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a. Read the iris dataset from the given url and assign it to Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('https://raw.githubusercontent.com/mpourhoma/CS4661/master/iris.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b. Split the dataset into testing and training sets test_size = 0.4, random_state = 10 with the following steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# b.1: create the feature matrix for iris dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.1.1: create X = data from all features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "\n",
    "X = iris_df[feature_cols]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b.1.2: create y =  serious of labels data (last column or label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = iris_df['species'] #non-numerical labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## b.2. Import train_test_split and split dataset with test_size = 0.4, random_state = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c. Instantiate a KNN object with k=3, then train it on the training set and test it on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "my_knn_for_hw2 = KNeighborsClassifier(n_neighbors=k) # name of the object is arbitrary!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.1 Training ONLY on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "                     metric_params=None, n_jobs=None, n_neighbors=3, p=2,\n",
       "                     weights='uniform')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_knn_for_hw2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## c.2 Testing on the testing set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_predict = my_knn_for_hw2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# c.3 We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Function \"accuracy_score\" from \"sklearn.metrics\" will perform the element-to-element comparision and returns the \n",
    "# portion of correct predictions:\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_predict)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# d. repeat part c (c, c1,c2,c3) for k = 1, 5, 7, 15, 27, 59 and add all accuracy to the list to check if it is getting better result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q: Does the accuracy always get better by increasing the value K?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A: No. The accuracy is not always get better even if we increase value of k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  1  - Accuracy:  0.9166666666666666\n",
      "k:  5  - Accuracy:  0.95\n",
      "k:  7  - Accuracy:  0.9666666666666667\n",
      "k:  15  - Accuracy:  0.95\n",
      "k:  27  - Accuracy:  0.95\n",
      "k:  59  - Accuracy:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "kList = [1, 5, 7, 15, 27, 59]\n",
    "\n",
    "# accuracy list\n",
    "accrList = []\n",
    "\n",
    "# loop through kList and perform all the following step with each k at each step\n",
    "for k in kList:\n",
    "    # set n_neightbors\n",
    "    my_knn_for_hw2 = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # training\n",
    "    my_knn_for_hw2.fit(X_train, y_train)\n",
    "    \n",
    "    # predict lable\n",
    "    y_predict = my_knn_for_hw2.predict(X_test)\n",
    "    \n",
    "    # calculate accuracy\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    \n",
    "    # add to accuracy list\n",
    "    accrList.append(accuracy)\n",
    "\n",
    "# print accuracy list out\n",
    "for k in range(len(kList)):\n",
    "    print(\"k: \", kList[k], \" - Accuracy: \", accrList[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# e. Make prediction based on One single feature with k = 11."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal_length - Accuracy:  0.6166666666666667\n",
      "sepal_width - Accuracy:  0.5333333333333333\n",
      "petal_length - Accuracy:  0.9333333333333333\n",
      "petal_width - Accuracy:  0.95\n",
      "\n",
      "Best feature is:  petal_width  - Accuracy:  0.95\n",
      "Second best feature is:  petal_length  - Accuracy:  0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# set k = 11 and set new n-neightbors\n",
    "k = 11\n",
    "my_knn_for_hw2 = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "feature_cols = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "\n",
    "# single feature accuracy list\n",
    "singleFeatureAccrList = []\n",
    "\n",
    "# loop through feature_cols with one feature at a time\n",
    "for f in feature_cols:\n",
    "    \n",
    "    # assign a signle feature\n",
    "    singleFeature = [f]\n",
    "    \n",
    "    # let X = data from only one single feature\n",
    "    X = iris_df[singleFeature] \n",
    "    \n",
    "    # split the dataset \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=10)\n",
    "    \n",
    "    # training\n",
    "    my_knn_for_hw2.fit(X_train, y_train)\n",
    "    \n",
    "    # predicting\n",
    "    y_predict = my_knn_for_hw2.predict(X_test)\n",
    "    \n",
    "    # caculating the accuracy\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    \n",
    "    # add to single feature accuracy list\n",
    "    singleFeatureAccrList.append(accuracy)\n",
    "\n",
    "# print the single feature accuracy list with corresponding feature name    \n",
    "for i in range(4):\n",
    "    print(feature_cols[i] + \" - Accuracy: \", singleFeatureAccrList[i])\n",
    "\n",
    "# print the best feature with the best accuracy    \n",
    "print(\"\\nBest feature is: \",feature_cols[singleFeatureAccrList.index(max(singleFeatureAccrList))], \n",
    "        \" - Accuracy: \",max(singleFeatureAccrList))  \n",
    "\n",
    "singleFeatureAccrList.remove(max(singleFeatureAccrList))\n",
    "\n",
    "# print the second best feature with the second best accuracy\n",
    "print(\"Second best feature is: \",feature_cols[singleFeatureAccrList.index(max(singleFeatureAccrList))], \n",
    "        \" - Accuracy: \",max(singleFeatureAccrList))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# f. Make prediction based on \"feature pair\" with k = 11 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal_length', 'sepal_width']  - Accuracy:  0.7333333333333333\n",
      "['sepal_length', 'petal_length']  - Accuracy:  0.9166666666666666\n",
      "['sepal_length', 'petal_width']  - Accuracy:  0.9\n",
      "['sepal_width', 'petal_length']  - Accuracy:  0.95\n",
      "['sepal_width', 'petal_width']  - Accuracy:  0.9666666666666667\n",
      "['petal_length', 'petal_width']  - Accuracy:  0.95\n",
      "\n",
      "Best feature pair is:  ['sepal_width', 'petal_width']  - Accuracy:  0.9666666666666667\n"
     ]
    }
   ],
   "source": [
    "k = 11\n",
    "my_knn_for_hw2 = KNeighborsClassifier(n_neighbors=k)\n",
    "feature_cols = ['sepal_length','sepal_width','petal_length','petal_width']\n",
    "twoFeaturesAccrList = []\n",
    "\n",
    "# create two features array \n",
    "twoFeatureCols = [\n",
    "    ['sepal_length', 'sepal_width'],\n",
    "    ['sepal_length', 'petal_length'],\n",
    "    ['sepal_length', 'petal_width'],\n",
    "    ['sepal_width', 'petal_length'],\n",
    "    ['sepal_width', 'petal_width'],\n",
    "    ['petal_length', 'petal_width']\n",
    "]\n",
    "\n",
    "# loop through two features array\n",
    "for f in twoFeatureCols:\n",
    "    # set X = data from each \"feature pair\" from two features array\n",
    "    X = iris_df[f]\n",
    "    \n",
    "    # split the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=10)\n",
    "    \n",
    "    # training\n",
    "    my_knn_for_hw2.fit(X_train, y_train)\n",
    "    \n",
    "    # prediction\n",
    "    y_predict = my_knn_for_hw2.predict(X_test)\n",
    "    \n",
    "    # calculating accuracy\n",
    "    accuracy = accuracy_score(y_test, y_predict)\n",
    "    \n",
    "    # add to two feature accuracy list\n",
    "    twoFeaturesAccrList.append(accuracy)\n",
    "\n",
    "# print the accuracies corresponding with each \"feature pair\"\n",
    "for i in range(6):\n",
    "    print(twoFeatureCols[i], \" - Accuracy: \", twoFeaturesAccrList[i])\n",
    "\n",
    "# print the best feature pair with the best accuracy\n",
    "print(\"\\nBest feature pair is: \",twoFeatureCols[twoFeaturesAccrList.index(max(twoFeaturesAccrList))], \n",
    "        \" - Accuracy: \",max(twoFeaturesAccrList))  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# g. Big Question.\n",
    "\n",
    "### Q1. Does the “best feature pair” from part (f) contain of both “first best feature” and “second best feature” from part (e)? \n",
    "\n",
    "### A1: No. The first and second best feature of e are petal_width and petal_length,  while the best feature pair in f is \"sepal_width - petal_width\"\n",
    "\n",
    "### Q2. In other word, can we conclude that the “best two features” for classification are the first best feature along with the second best feature together?\n",
    "\n",
    "### A2: No."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# h. Bonus (Optional) Question\n",
    "\n",
    "### We cannot conclude that the \"best two features\" for classification includes the first and second best feature when we train the model with single feature only.\n",
    "### This is because when we combine 2 feature to train, then it will affect our train model. \n",
    "### In other words, when we train the best and second best feature together, they might affect each other somehow, then the accuracy of combination between them might be not as good as the combination of the two other ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
